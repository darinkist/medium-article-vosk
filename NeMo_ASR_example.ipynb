{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeMo_ASR_example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyME8INgvK7GejIx+5f/7MrR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darinkist/medium_article_vosk/blob/main/NeMo_ASR_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_h2a10J650P"
      },
      "outputs": [],
      "source": [
        "# Credits: https://github.com/NVIDIA/NeMo/blob/c9d04851e8a9c1382326862126788fadd77663ac/tutorials/asr/Streaming_ASR.ipynb\n",
        "# Install dependencies\n",
        "!pip install nemo_toolkit[all]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import nemo.collections.asr as nemo_asr\n",
        "from nemo.core.classes import IterableDataset\n",
        "import numpy as np\n",
        "import math\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "\n",
        "# To get an overview of existing pre-trained models run nemo_asr.models.EncDecCTCModelBPE.list_available_models()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(\"stt_en_conformer_ctc_large\", map_location=device)"
      ],
      "metadata": {
        "id": "fSUxYmZK7OFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple iterator class to return successive chunks of samples\n",
        "class AudioChunkIterator():\n",
        "    def __init__(self, samples, frame_len, sample_rate):\n",
        "        self._samples = samples\n",
        "        self._chunk_len = chunk_len_in_secs*sample_rate\n",
        "        self._start = 0\n",
        "        self.output=True\n",
        "   \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if not self.output:\n",
        "            raise StopIteration\n",
        "        last = int(self._start + self._chunk_len)\n",
        "        if last <= len(self._samples):\n",
        "            chunk = self._samples[self._start: last]\n",
        "            self._start = last\n",
        "        else:\n",
        "            chunk = np.zeros([int(self._chunk_len)], dtype='float32')\n",
        "            samp_len = len(self._samples) - self._start\n",
        "            chunk[0:samp_len] = self._samples[self._start:len(self._samples)]\n",
        "            self.output = False\n",
        "   \n",
        "        return chunk\n",
        "\n",
        "# a helper function for extracting samples as a numpy array from the audio file\n",
        "def get_samples(audio_file, target_sr=16000):\n",
        "    with sf.SoundFile(audio_file, 'r') as f:\n",
        "        dtype = 'int16'\n",
        "        sample_rate = f.samplerate\n",
        "        samples = f.read(dtype=dtype)\n",
        "        if sample_rate != target_sr:\n",
        "            samples = librosa.core.resample(samples, sample_rate, target_sr)\n",
        "        samples=samples.astype('float32')/32768\n",
        "        samples = samples.transpose()\n",
        "        return samples\n",
        "\n",
        "\n",
        "def speech_collate_fn(batch):\n",
        "    \"\"\"collate batch of audio sig, audio len\n",
        "    Args:\n",
        "        batch (FloatTensor, LongTensor):  A tuple of tuples of signal, signal lengths.\n",
        "        This collate func assumes the signals are 1d torch tensors (i.e. mono audio).\n",
        "    \"\"\"\n",
        "\n",
        "    _, audio_lengths = zip(*batch)\n",
        "\n",
        "    max_audio_len = 0\n",
        "    has_audio = audio_lengths[0] is not None\n",
        "    if has_audio:\n",
        "        max_audio_len = max(audio_lengths).item()\n",
        "   \n",
        "    \n",
        "    audio_signal= []\n",
        "    for sig, sig_len in batch:\n",
        "        if has_audio:\n",
        "            sig_len = sig_len.item()\n",
        "            if sig_len < max_audio_len:\n",
        "                pad = (0, max_audio_len - sig_len)\n",
        "                sig = torch.nn.functional.pad(sig, pad)\n",
        "            audio_signal.append(sig)\n",
        "        \n",
        "    if has_audio:\n",
        "        audio_signal = torch.stack(audio_signal)\n",
        "        audio_lengths = torch.stack(audio_lengths)\n",
        "    else:\n",
        "        audio_signal, audio_lengths = None, None\n",
        "\n",
        "    return audio_signal, audio_lengths\n",
        "\n",
        "\n",
        "# simple data layer to pass audio signal\n",
        "class AudioBuffersDataLayer(IterableDataset):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self._buf_count == len(self.signal) :\n",
        "            raise StopIteration\n",
        "        self._buf_count +=1\n",
        "        return torch.as_tensor(self.signal[self._buf_count-1], dtype=torch.float32), \\\n",
        "               torch.as_tensor(self.signal_shape[0], dtype=torch.int64)\n",
        "        \n",
        "    def set_signal(self, signals):\n",
        "        self.signal = signals\n",
        "        self.signal_shape = self.signal[0].shape\n",
        "        self._buf_count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "\n",
        "class ChunkBufferDecoder:\n",
        "\n",
        "    def __init__(self,asr_model, stride, chunk_len_in_secs=1, buffer_len_in_secs=3):\n",
        "        self.asr_model = asr_model\n",
        "        self.asr_model.eval()\n",
        "        self.data_layer = AudioBuffersDataLayer()\n",
        "        self.data_loader = DataLoader(self.data_layer, batch_size=1, collate_fn=speech_collate_fn)\n",
        "        self.buffers = []\n",
        "        self.all_preds = []\n",
        "        self.chunk_len = chunk_len_in_secs\n",
        "        self.buffer_len = buffer_len_in_secs\n",
        "        assert(chunk_len_in_secs<=buffer_len_in_secs)\n",
        "        \n",
        "        feature_stride = asr_model._cfg.preprocessor['window_stride']\n",
        "        self.model_stride_in_secs = feature_stride * stride\n",
        "        self.n_tokens_per_chunk = math.ceil(self.chunk_len / self.model_stride_in_secs)\n",
        "        self.blank_id = len(asr_model.decoder.vocabulary)\n",
        "        \n",
        "    @torch.no_grad()    \n",
        "    def transcribe_buffers(self, buffers, merge=True):\n",
        "        self.buffers = buffers\n",
        "        self.data_layer.set_signal(buffers[:])\n",
        "        self._get_batch_preds()      \n",
        "        return self.decode_final(merge)\n",
        "    \n",
        "    def _get_batch_preds(self):\n",
        "\n",
        "        device = self.asr_model.device\n",
        "        for batch in iter(self.data_loader):\n",
        "\n",
        "            audio_signal, audio_signal_len = batch\n",
        "\n",
        "            audio_signal, audio_signal_len = audio_signal.to(device), audio_signal_len.to(device)\n",
        "            log_probs, encoded_len, predictions = self.asr_model(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
        "            preds = torch.unbind(predictions)\n",
        "            for pred in preds:\n",
        "                self.all_preds.append(pred.cpu().numpy())\n",
        "    \n",
        "    def decode_final(self, merge=True, extra=0):\n",
        "        self.unmerged = []\n",
        "        self.toks_unmerged = []\n",
        "        # index for the first token corresponding to a chunk of audio would be len(decoded) - 1 - delay\n",
        "        delay = math.ceil((self.chunk_len + (self.buffer_len - self.chunk_len) / 2) / self.model_stride_in_secs)\n",
        "\n",
        "        decoded_frames = []\n",
        "        all_toks = []\n",
        "        for pred in self.all_preds:\n",
        "            ids, toks = self._greedy_decoder(pred, self.asr_model.tokenizer)\n",
        "            decoded_frames.append(ids)\n",
        "            all_toks.append(toks)\n",
        "\n",
        "        for decoded in decoded_frames:\n",
        "            self.unmerged += decoded[len(decoded) - 1 - delay:len(decoded) - 1 - delay + self.n_tokens_per_chunk]\n",
        "\n",
        "        if not merge:\n",
        "            return self.unmerged\n",
        "        return self.greedy_merge(self.unmerged)\n",
        "    \n",
        "    \n",
        "    def _greedy_decoder(self, preds, tokenizer):\n",
        "        s = []\n",
        "        ids = []\n",
        "        for i in range(preds.shape[0]):\n",
        "            if preds[i] == self.blank_id:\n",
        "                s.append(\"_\")\n",
        "            else:\n",
        "                pred = preds[i]\n",
        "                s.append(tokenizer.ids_to_tokens([pred.item()])[0])\n",
        "            ids.append(preds[i])\n",
        "        return ids, s\n",
        "         \n",
        "    def greedy_merge(self, preds):\n",
        "        decoded_prediction = []\n",
        "        previous = self.blank_id\n",
        "        for p in preds:\n",
        "            if (p != previous or previous == self.blank_id) and p != self.blank_id:\n",
        "                decoded_prediction.append(p.item())\n",
        "            previous = p\n",
        "        hypothesis = self.asr_model.tokenizer.ids_to_text(decoded_prediction)\n",
        "        return hypothesis"
      ],
      "metadata": {
        "id": "1KoSH1JZ8TSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = '/content/opto_sessions_ep_69_excerpt.wav' # wav 16bit mono\n",
        "\n",
        "samples = get_samples(audio_path)\n",
        "sample_rate  = model.preprocessor._cfg['sample_rate'] \n",
        "\n",
        "chunk_len_in_secs = 8\n",
        "context_len_in_secs = 4\n",
        "\n",
        "buffer_len_in_secs = chunk_len_in_secs + 2* context_len_in_secs\n",
        "\n",
        "buffer_len = sample_rate*buffer_len_in_secs\n",
        "sampbuffer = np.zeros([buffer_len], dtype=np.float32)\n",
        "\n",
        "chunk_reader = AudioChunkIterator(samples, chunk_len_in_secs, sample_rate)\n",
        "chunk_len = sample_rate*chunk_len_in_secs\n",
        "count = 0\n",
        "buffer_list = []\n",
        "\n",
        "for chunk in chunk_reader:\n",
        "    count +=1\n",
        "    sampbuffer[:-chunk_len] = sampbuffer[chunk_len:]\n",
        "    sampbuffer[-chunk_len:] = chunk\n",
        "    buffer_list.append(np.array(sampbuffer))\n",
        "\n",
        "stride = 4 # 8 for Citrinet\n",
        "asr_decoder = ChunkBufferDecoder(model, stride=stride, chunk_len_in_secs=chunk_len_in_secs, buffer_len_in_secs=buffer_len_in_secs )\n",
        "transcription = asr_decoder.transcribe_buffers(buffer_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heKt70hX81PA",
        "outputId": "cc4809d7-f756-4dc7-fc81-b8ec8cbdbbd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-01-03 14:45:57 nemo_logging:349] /usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "      warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcription"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "oPH_ikAA-HrN",
        "outputId": "eed7127a-279b-4008-8acb-8aea1177b6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"to success on today's show i'm delighted to introduce beth kindig a technology analyst with over a decade of experience in the private markets she is now the co founder of io fund which specializes in helping individuals gain a competitive advantage when investing in tech gross stocks how does beth do this well she's gained hands on experience over the years whilst either working for and or analyzing a huge amount of relevant tech companies in s\""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}